<!DOCTYPE html>
<html lang="en">
 <head>
  <title>Response to AI and Accessibility</title>
  <style>
   body { background: cornsilk; }
   h1 { background: bisque; color: darkorchid; font-weight: bold; }
   p { background: cornsilk; }
  </style>
 </head>
 <body>
<p style="font-weight:bold;font-size:larger;text-align:center; color:darkorchid">
   For the Neurodivergent Employee or the Capitalist Employer?
  </p>
  <p style="color:black">
   The article “How AI is Improving Accessibility” advertises ways recent uses of AI have helped neurodivergent users specifically. The text highlights its place in the classroom and workplace, wholly being in favor of these improvements. The improvements AI has made for helping neurodivergent children learn are relevant and meaningful to the education landscape as another tool for helping children reach their full potential. 
   <strong>
   However, it can also be said that these same tools have the potential to be used to exploit their usefulness to remove learning and self-improvement in classrooms and workplaces.
   </strong>
  </p>
  <p style="color:black">
   First, one of the sources The Neuro used in writing their article was a Natalie Marchant article on educational technology (edtech) post COVID-19. The technology described in this article was more in line with how AI and other advanced technologies can improve education by supporting students, not removing the need for education. For example, AI-powered homework websites that provide personalized help for homework provide a core component of quality education (i.e. personalization), but they do not change the need for the students to learn the necessary knowledge of skills designed in the class. Compare this to the description of Salesforce’s summary feature for long texts. While this development likely helps neurodivergent workers as it claims to, the feature also ignores the intentions of the original author’s words, especially if those who could read the entire text choose not to only to save time. 
  </p>
  <p style="color:black">
   This issue leads to my further concern with articles like the one read today that support the use of AI and new technology without any criticism or qualifiers to its support. Many of the articles and websites that explain the benefits of technology often come from technology or finance industries that thrive on capitalist mindsets, which promote high specialization and little individualized artistic thought. Thus, their description of the technologies that “improve” accessibility in education focus on how it makes students do less work, need to think less critically because the AI can summarize or write something for you, etc. 
   <strong>
   This is in contrast to existing technology that, as described in the Marchant’s article, supports and facilitates education, rather than replacing it. 
   </strong>
   However, through a corporate lens, the outcome is the same (workers who can complete goals for them), so removing intrinsic motivation to learn and making individuals capitalist robots is irrelevant to their bottom line and net profit.
  </p>
  <p style="color:black">
   The question of corporation’s motivations comes into play again when we consider the article’s brief but telling explanation of why companies should hire neurodivergent employees: it’s “efficient”, they are less likely to leave, they don’t suffer from burnout as easily, they like repetition. It sounds like a cold and sinister way to look at inclusion and is representative of a larger plastic feeling to the article presented here. 
   <strong>
   The article is framing AI to market it to readers, making it clear that its goal is to support the tech industry, not the neurodivergent worker. 
   </strong>
  </p>
  <p style="color:black">
   From this article, I have had a few questions come mind that don’t have clear answers but I think are important to consider when we evaluate how to use AI going forward and what areas need improvement: 
   <ul>
    <li> Is there any kind of method of improving accessibility that boosts those who need the help while not letting others slack off? In other words, is there any way to build technology that is not exploitable? </li>
    <li> Where is the line between meeting accommodations/improving accessibility and lowering standards of achievements/expectations? </li>
    <li> Is there a way to use AI that does not remove the need and appreciation in our society/social culture for artistic expression and personal voice? </li>
    <li> While new technology is intended to close the attainment gap for those who have access to it, the access itself is separated and would increase gaps based on SES and school districts? Is there a way to ethically introduce technology to public schooling that wouldn’t increase disparity? </li>
   </ul>
  </p>
 </body>
